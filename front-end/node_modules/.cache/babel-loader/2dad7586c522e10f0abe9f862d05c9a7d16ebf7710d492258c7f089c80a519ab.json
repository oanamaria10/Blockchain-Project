{"ast":null,"code":"import * as API from './api.js';\nimport { IN_BYTES_PER_QUAD, IN_BITS_FR, OUT_BITS_FR, MIN_PAYLOAD_SIZE } from './constant.js';\nimport * as ZeroPad from './zero-comm.js';\nimport { computeNode } from './proof.js';\nimport { split } from './piece/tree.js';\nimport { pad } from './fr32.js';\nimport { fromHeight as piceSizeFromHeight } from './piece/size/expanded.js';\nimport { Unpadded } from './piece/size.js';\nimport * as Digest from './digest.js';\nimport { varint } from 'multiformats';\nexport { Digest };\n\n/**\n * @see https://github.com/multiformats/multicodec/pull/331/files\n */\nexport const name = /** @type {const} */\n'fr32-sha2-256-trunc254-padded-binary-tree';\n\n/**\n * @type {API.MulticodecCode<0x1011, typeof name>}\n * @see https://github.com/multiformats/multicodec/pull/331/files\n */\nexport const code = 0x1011;\n\n/**\n * Since first byte in the digest is the tree height, the maximum height is 255.\n *\n * @type {255}\n */\nexport const MAX_HEIGHT = 255;\n\n/**\n * Max payload is determined by the maximum height of the tree, which is limited\n * by the int we could store in one byte. We calculate the max piece size\n * and derive max payload size that can would produce it after FR32 padding.\n */\nexport const MAX_PAYLOAD_SIZE = piceSizeFromHeight(MAX_HEIGHT) * BigInt(IN_BITS_FR) / BigInt(OUT_BITS_FR);\n\n/**\n * Computes the digest of the given payload.\n *\n * @param {Uint8Array} payload\n * @returns {API.PieceDigest}\n */\nexport const digest = payload => {\n  const hasher = new Hasher();\n  hasher.write(payload);\n  return hasher.digest();\n};\n\n/**\n * Creates a streaming hasher that can be used to consumer larger streams\n * of data than it would be practical to load into memory all at once.\n *\n * @returns {API.StreamingHasher<typeof code, number, API.PieceDigest>}\n */\nexport const create = () => new Hasher();\n\n/**\n * @typedef {[API.MerkleTreeNode[], ...API.MerkleTreeNode[][]]} Layers\n *\n * @implements {API.StreamingHasher<typeof code, number, API.PieceDigest>}\n */\nclass Hasher {\n  constructor() {\n    /**\n     * The number of bytes consumed by the hasher.\n     *\n     * @private\n     */\n    this.bytesWritten = 0n;\n\n    /**\n     * This buffer is used to accumulate bytes until we have enough to fill a\n     * quad.\n     *\n     * ⚠️ Note that you should never read bytes past {@link offset} as those\n     * are considered dirty and may contain garbage.\n     *\n     * @protected\n     */\n    this.buffer = new Uint8Array(IN_BYTES_PER_QUAD);\n\n    /**\n     * Offset is the number of bytes in we have written into the buffer. If\n     * offset is 0 it means that the buffer is effectively empty. When `offset`\n     * is equal to `this.buffer.length` we have a quad that can be processed.\n     *\n     * @protected\n     */\n    this.offset = 0;\n\n    /**\n     * The layers of the tree. Each layer will contain either 0 or 1 nodes\n     * between writes. When we write into a hasher, if we have enough nodes\n     * leaves will be created and pushed into the `layers[0]` array, after\n     * which we flush and combine every two leafs into a node which is moved\n     * to the next layer. This process is repeated until we reach the top\n     * layer, leaving each layer either empty or with a single node.\n     *\n     * @type {Layers}\n     */\n    this.layers = [[]];\n  }\n\n  /**\n   * Return the total number of bytes written into the hasher. Calling\n   * {@link reset} will reset the hasher and the count will be reset to 0.\n   *\n   * @returns {bigint}\n   */\n  count() {\n    return this.bytesWritten;\n  }\n\n  /**\n   * Computes the digest of all the data that has been written into this hasher.\n   * This method does not have side-effects, meaning that you can continue\n   * writing and call this method again to compute digest of all the data\n   * written from the very beginning.\n   */\n  digest() {\n    const bytes = new Uint8Array(Digest.MAX_SIZE);\n    const count = this.digestInto(bytes, 0, true);\n    return Digest.fromBytes(bytes.subarray(0, count));\n  }\n\n  /**\n   * Computes the digest and writes into the given buffer. You can provide\n   * optional `byteOffset` to write digest at that offset in the buffer. By\n   * default the multihash prefix will be written into the buffer, but you can\n   * opt-out by passing `false` as the `asMultihash` argument.\n   *\n   * @param {Uint8Array} output\n   * @param {number} [byteOffset]\n   * @param {boolean} asMultihash\n   */\n  digestInto(output, byteOffset = 0, asMultihash = true) {\n    const {\n      buffer,\n      layers,\n      offset,\n      bytesWritten\n    } = this;\n\n    // We do not want to mutate the layers, so we create a shallow copy of it\n    // which we will use to compute the root.\n    let [leaves, ...nodes] = layers;\n\n    // If we have some bytes in the buffer we fill rest with zeros and compute\n    // leaves from it. Note that it is safe to mutate the buffer here as bytes\n    // past `offset` are considered dirty and should not be read.\n    if (offset > 0 || bytesWritten === 0n) {\n      leaves = [...leaves, ...split(pad(buffer.fill(0, offset)))];\n    }\n    const tree = build([leaves, ...nodes]);\n    const height = tree.length - 1;\n    const [root] = tree[height];\n    const padding = Number(Unpadded.toPadding(this.bytesWritten));\n    const paddingLength = varint.encodingLength( /** @type {number & bigint} */padding);\n    let endOffset = byteOffset;\n    // Write the multihash prefix if requested\n    if (asMultihash) {\n      varint.encodeTo(code, output, endOffset);\n      endOffset += Digest.TAG_SIZE;\n      const size = paddingLength + Digest.HEIGHT_SIZE + Digest.ROOT_SIZE;\n      const sizeLength = varint.encodingLength(size);\n      varint.encodeTo(size, output, endOffset);\n      endOffset += sizeLength;\n    }\n    varint.encodeTo(padding, output, endOffset);\n    endOffset += paddingLength;\n\n    // Write the tree height as the first byte of the digest\n    output[endOffset] = height;\n    endOffset += 1;\n\n    // Write the root as the remaining 32 bytes of the digest\n    output.set(root, endOffset);\n    endOffset += root.length;\n\n    // Return number of bytes written\n    return endOffset - byteOffset;\n  }\n  /**\n   * @param {Uint8Array} bytes\n   */\n  write(bytes) {\n    const {\n      buffer,\n      offset,\n      layers\n    } = this;\n    const leaves = layers[0];\n    const {\n      length\n    } = bytes;\n    // If we got no bytes there is nothing to do here\n    if (length === 0) {\n      return this;\n      /* c8 ignore next 5 */\n    } else if (this.bytesWritten + BigInt(length) > MAX_PAYLOAD_SIZE) {\n      throw new RangeError(`Writing ${length} bytes exceeds max payload size of ${MAX_PAYLOAD_SIZE}`);\n    }\n    // If we do not have enough bytes to form a quad, just add append new bytes\n    // to the buffer and return.\n    else if (offset + length < buffer.length) {\n      buffer.set(bytes, offset);\n      this.offset += length;\n      this.bytesWritten += BigInt(length);\n      return this;\n    }\n    // Otherwise we first fill the buffer to form a quad and create some leaves.\n    // Then we slice remaining bytes into quads sized chunks and create leaves\n    // from them. If we have some bytes left we copy them into the buffer and\n    // flush to combining node pairs and propagate them up the tree.\n    else {\n      // Number of bytes required to fill the quad buffer\n      const bytesRequired = buffer.length - offset;\n      // copy required bytes into the buffer and turn them into leaves\n      // which we push into the leaf layer.\n      buffer.set(bytes.subarray(0, bytesRequired), offset);\n      leaves.push(...split(pad(buffer)));\n\n      // Now we slice remaining bytes into quads, create leaves from them\n      // and push them into the leaf layer.\n      let readOffset = bytesRequired;\n      while (readOffset + IN_BYTES_PER_QUAD < length) {\n        const quad = bytes.subarray(readOffset, readOffset + IN_BYTES_PER_QUAD);\n        leaves.push(...split(pad(quad)));\n        readOffset += IN_BYTES_PER_QUAD;\n      }\n\n      // Whatever byte were left are copied into the buffer and we update\n      // the offset to reflect that.\n      this.buffer.set(bytes.subarray(readOffset), 0);\n      this.offset = length - readOffset;\n\n      // We also update the total number of bytes written.\n      this.bytesWritten += BigInt(length);\n\n      // Now prune the layers to propagate all the new leaves up the tree.\n      prune(this.layers);\n      return this;\n    }\n  }\n\n  /**\n   * Resets this hasher to its initial state so it could be recycled as new\n   * instance.\n   */\n  reset() {\n    this.offset = 0;\n    this.bytesWritten = 0n;\n    this.layers.length = 1;\n    this.layers[0].length = 0;\n    return this;\n  }\n\n  /* c8 ignore next 3 */\n  dispose() {\n    this.reset();\n  }\n  get code() {\n    return code;\n  }\n  get name() {\n    return name;\n  }\n}\n\n/**\n * Prunes layers by combining node pairs into nodes in the next layer and\n * removing them from the layer that they were in. After pruning each layer\n * will end up with at most one node. New layers may be created in the process\n * when nodes from the top layer are combined.\n *\n * @param {Layers} layers\n */\nconst prune = layers => flush(layers, false);\n\n/**\n * Flushes all the nodes in layers by combining node pairs into nodes in the\n * next layer. Layers with only one node are combined with zero padded nodes\n * (corresponding to the level of the layer). Unlike {@link prune} combined\n * nodes are not removed and layers are copied instead of been mutated.\n *\n * @param {Layers} layers\n */\nconst build = layers => flush([...layers], true);\n\n/**\n * @param {Layers} layers\n * @param {boolean} build\n * @returns {Layers}\n */\nconst flush = (layers, build) => {\n  // Note it is important that we do not mutate any of the layers otherwise\n  // writing more data into the hasher and computing the digest will produce\n  // wrong results.\n  let level = 0;\n  // We will walk up the tree until we reach the top layer. However, we may end\n  // up with creating new layers in the process, so we will keep track of the\n  while (level < layers.length) {\n    let next = layers[level + 1];\n    const layer = layers[level];\n\n    // If we have the odd number of nodes and we have not reached the top\n    // layer, we push a zero padding node corresponding to the current level.\n    if (build && layer.length % 2 > 0 && next) {\n      layer.push(ZeroPad.fromLevel(level));\n    }\n    level += 1;\n\n    // If we have 0 nodes in the current layer we just move to the next one.\n\n    // If we have a next layer and we are building  will combine nodes from the current layer\n    next = next ? build ? [...next] : next : [];\n    let index = 0;\n    // Note that we have checked that we have an even number of nodes so\n    // we will never end up with an extra node when consuming two at a time.\n    while (index + 1 < layer.length) {\n      const node = computeNode(layer[index], layer[index + 1]);\n\n      // we proactively delete nodes in order to free up a memory used.\n      delete layer[index];\n      delete layer[index + 1];\n      next.push(node);\n      index += 2;\n    }\n    if (next.length) {\n      layers[level] = next;\n    }\n\n    // we remove nodes that we have combined from the current layer to reduce\n    // memory overhead and move to the next layer.\n    layer.splice(0, index);\n  }\n  return layers;\n};","map":{"version":3,"names":["API","IN_BYTES_PER_QUAD","IN_BITS_FR","OUT_BITS_FR","MIN_PAYLOAD_SIZE","ZeroPad","computeNode","split","pad","fromHeight","piceSizeFromHeight","Unpadded","Digest","varint","name","code","MAX_HEIGHT","MAX_PAYLOAD_SIZE","BigInt","digest","payload","hasher","Hasher","write","create","constructor","bytesWritten","buffer","Uint8Array","offset","layers","count","bytes","MAX_SIZE","digestInto","fromBytes","subarray","output","byteOffset","asMultihash","leaves","nodes","fill","tree","build","height","length","root","padding","Number","toPadding","paddingLength","encodingLength","endOffset","encodeTo","TAG_SIZE","size","HEIGHT_SIZE","ROOT_SIZE","sizeLength","set","RangeError","bytesRequired","push","readOffset","quad","prune","reset","dispose","flush","level","next","layer","fromLevel","index","node","splice"],"sources":["C:/FACULTATE/Proiect/node_modules/@web3-storage/data-segment/src/multihash.js"],"sourcesContent":["import * as API from './api.js'\nimport {\n  IN_BYTES_PER_QUAD,\n  IN_BITS_FR,\n  OUT_BITS_FR,\n  MIN_PAYLOAD_SIZE,\n} from './constant.js'\nimport * as ZeroPad from './zero-comm.js'\nimport { computeNode } from './proof.js'\nimport { split } from './piece/tree.js'\nimport { pad } from './fr32.js'\nimport { fromHeight as piceSizeFromHeight } from './piece/size/expanded.js'\nimport { Unpadded } from './piece/size.js'\nimport * as Digest from './digest.js'\nimport { varint } from 'multiformats'\n\nexport { Digest }\n\n/**\n * @see https://github.com/multiformats/multicodec/pull/331/files\n */\nexport const name = /** @type {const} */ (\n  'fr32-sha2-256-trunc254-padded-binary-tree'\n)\n\n/**\n * @type {API.MulticodecCode<0x1011, typeof name>}\n * @see https://github.com/multiformats/multicodec/pull/331/files\n */\nexport const code = 0x1011\n\n/**\n * Since first byte in the digest is the tree height, the maximum height is 255.\n *\n * @type {255}\n */\nexport const MAX_HEIGHT = 255\n\n/**\n * Max payload is determined by the maximum height of the tree, which is limited\n * by the int we could store in one byte. We calculate the max piece size\n * and derive max payload size that can would produce it after FR32 padding.\n */\nexport const MAX_PAYLOAD_SIZE =\n  (piceSizeFromHeight(MAX_HEIGHT) * BigInt(IN_BITS_FR)) / BigInt(OUT_BITS_FR)\n\n/**\n * Computes the digest of the given payload.\n *\n * @param {Uint8Array} payload\n * @returns {API.PieceDigest}\n */\nexport const digest = (payload) => {\n  const hasher = new Hasher()\n  hasher.write(payload)\n  return hasher.digest()\n}\n\n/**\n * Creates a streaming hasher that can be used to consumer larger streams\n * of data than it would be practical to load into memory all at once.\n *\n * @returns {API.StreamingHasher<typeof code, number, API.PieceDigest>}\n */\nexport const create = () => new Hasher()\n\n/**\n * @typedef {[API.MerkleTreeNode[], ...API.MerkleTreeNode[][]]} Layers\n *\n * @implements {API.StreamingHasher<typeof code, number, API.PieceDigest>}\n */\nclass Hasher {\n  constructor() {\n    /**\n     * The number of bytes consumed by the hasher.\n     *\n     * @private\n     */\n    this.bytesWritten = 0n\n\n    /**\n     * This buffer is used to accumulate bytes until we have enough to fill a\n     * quad.\n     *\n     * ⚠️ Note that you should never read bytes past {@link offset} as those\n     * are considered dirty and may contain garbage.\n     *\n     * @protected\n     */\n    this.buffer = new Uint8Array(IN_BYTES_PER_QUAD)\n\n    /**\n     * Offset is the number of bytes in we have written into the buffer. If\n     * offset is 0 it means that the buffer is effectively empty. When `offset`\n     * is equal to `this.buffer.length` we have a quad that can be processed.\n     *\n     * @protected\n     */\n    this.offset = 0\n\n    /**\n     * The layers of the tree. Each layer will contain either 0 or 1 nodes\n     * between writes. When we write into a hasher, if we have enough nodes\n     * leaves will be created and pushed into the `layers[0]` array, after\n     * which we flush and combine every two leafs into a node which is moved\n     * to the next layer. This process is repeated until we reach the top\n     * layer, leaving each layer either empty or with a single node.\n     *\n     * @type {Layers}\n     */\n    this.layers = [[]]\n  }\n\n  /**\n   * Return the total number of bytes written into the hasher. Calling\n   * {@link reset} will reset the hasher and the count will be reset to 0.\n   *\n   * @returns {bigint}\n   */\n  count() {\n    return this.bytesWritten\n  }\n\n  /**\n   * Computes the digest of all the data that has been written into this hasher.\n   * This method does not have side-effects, meaning that you can continue\n   * writing and call this method again to compute digest of all the data\n   * written from the very beginning.\n   */\n  digest() {\n    const bytes = new Uint8Array(Digest.MAX_SIZE)\n    const count = this.digestInto(bytes, 0, true)\n    return Digest.fromBytes(bytes.subarray(0, count))\n  }\n\n  /**\n   * Computes the digest and writes into the given buffer. You can provide\n   * optional `byteOffset` to write digest at that offset in the buffer. By\n   * default the multihash prefix will be written into the buffer, but you can\n   * opt-out by passing `false` as the `asMultihash` argument.\n   *\n   * @param {Uint8Array} output\n   * @param {number} [byteOffset]\n   * @param {boolean} asMultihash\n   */\n  digestInto(output, byteOffset = 0, asMultihash = true) {\n    const { buffer, layers, offset, bytesWritten } = this\n\n    // We do not want to mutate the layers, so we create a shallow copy of it\n    // which we will use to compute the root.\n    let [leaves, ...nodes] = layers\n\n    // If we have some bytes in the buffer we fill rest with zeros and compute\n    // leaves from it. Note that it is safe to mutate the buffer here as bytes\n    // past `offset` are considered dirty and should not be read.\n    if (offset > 0 || bytesWritten === 0n) {\n      leaves = [...leaves, ...split(pad(buffer.fill(0, offset)))]\n    }\n\n    const tree = build([leaves, ...nodes])\n    const height = tree.length - 1\n    const [root] = tree[height]\n    const padding = Number(Unpadded.toPadding(this.bytesWritten))\n\n    const paddingLength = varint.encodingLength(\n      /** @type {number & bigint} */ (padding)\n    )\n\n    let endOffset = byteOffset\n    // Write the multihash prefix if requested\n    if (asMultihash) {\n      varint.encodeTo(code, output, endOffset)\n      endOffset += Digest.TAG_SIZE\n\n      const size = paddingLength + Digest.HEIGHT_SIZE + Digest.ROOT_SIZE\n      const sizeLength = varint.encodingLength(size)\n      varint.encodeTo(size, output, endOffset)\n      endOffset += sizeLength\n    }\n\n    varint.encodeTo(padding, output, endOffset)\n    endOffset += paddingLength\n\n    // Write the tree height as the first byte of the digest\n    output[endOffset] = height\n    endOffset += 1\n\n    // Write the root as the remaining 32 bytes of the digest\n    output.set(root, endOffset)\n    endOffset += root.length\n\n    // Return number of bytes written\n    return endOffset - byteOffset\n  }\n  /**\n   * @param {Uint8Array} bytes\n   */\n  write(bytes) {\n    const { buffer, offset, layers } = this\n    const leaves = layers[0]\n    const { length } = bytes\n    // If we got no bytes there is nothing to do here\n    if (length === 0) {\n      return this\n      /* c8 ignore next 5 */\n    } else if (this.bytesWritten + BigInt(length) > MAX_PAYLOAD_SIZE) {\n      throw new RangeError(\n        `Writing ${length} bytes exceeds max payload size of ${MAX_PAYLOAD_SIZE}`\n      )\n    }\n    // If we do not have enough bytes to form a quad, just add append new bytes\n    // to the buffer and return.\n    else if (offset + length < buffer.length) {\n      buffer.set(bytes, offset)\n      this.offset += length\n      this.bytesWritten += BigInt(length)\n      return this\n    }\n    // Otherwise we first fill the buffer to form a quad and create some leaves.\n    // Then we slice remaining bytes into quads sized chunks and create leaves\n    // from them. If we have some bytes left we copy them into the buffer and\n    // flush to combining node pairs and propagate them up the tree.\n    else {\n      // Number of bytes required to fill the quad buffer\n      const bytesRequired = buffer.length - offset\n      // copy required bytes into the buffer and turn them into leaves\n      // which we push into the leaf layer.\n      buffer.set(bytes.subarray(0, bytesRequired), offset)\n      leaves.push(...split(pad(buffer)))\n\n      // Now we slice remaining bytes into quads, create leaves from them\n      // and push them into the leaf layer.\n      let readOffset = bytesRequired\n      while (readOffset + IN_BYTES_PER_QUAD < length) {\n        const quad = bytes.subarray(readOffset, readOffset + IN_BYTES_PER_QUAD)\n        leaves.push(...split(pad(quad)))\n        readOffset += IN_BYTES_PER_QUAD\n      }\n\n      // Whatever byte were left are copied into the buffer and we update\n      // the offset to reflect that.\n      this.buffer.set(bytes.subarray(readOffset), 0)\n      this.offset = length - readOffset\n\n      // We also update the total number of bytes written.\n      this.bytesWritten += BigInt(length)\n\n      // Now prune the layers to propagate all the new leaves up the tree.\n      prune(this.layers)\n\n      return this\n    }\n  }\n\n  /**\n   * Resets this hasher to its initial state so it could be recycled as new\n   * instance.\n   */\n  reset() {\n    this.offset = 0\n    this.bytesWritten = 0n\n    this.layers.length = 1\n    this.layers[0].length = 0\n    return this\n  }\n\n  /* c8 ignore next 3 */\n  dispose() {\n    this.reset()\n  }\n  get code() {\n    return code\n  }\n  get name() {\n    return name\n  }\n}\n\n/**\n * Prunes layers by combining node pairs into nodes in the next layer and\n * removing them from the layer that they were in. After pruning each layer\n * will end up with at most one node. New layers may be created in the process\n * when nodes from the top layer are combined.\n *\n * @param {Layers} layers\n */\nconst prune = (layers) => flush(layers, false)\n\n/**\n * Flushes all the nodes in layers by combining node pairs into nodes in the\n * next layer. Layers with only one node are combined with zero padded nodes\n * (corresponding to the level of the layer). Unlike {@link prune} combined\n * nodes are not removed and layers are copied instead of been mutated.\n *\n * @param {Layers} layers\n */\nconst build = (layers) => flush([...layers], true)\n\n/**\n * @param {Layers} layers\n * @param {boolean} build\n * @returns {Layers}\n */\nconst flush = (layers, build) => {\n  // Note it is important that we do not mutate any of the layers otherwise\n  // writing more data into the hasher and computing the digest will produce\n  // wrong results.\n  let level = 0\n  // We will walk up the tree until we reach the top layer. However, we may end\n  // up with creating new layers in the process, so we will keep track of the\n  while (level < layers.length) {\n    let next = layers[level + 1]\n    const layer = layers[level]\n\n    // If we have the odd number of nodes and we have not reached the top\n    // layer, we push a zero padding node corresponding to the current level.\n    if (build && layer.length % 2 > 0 && next) {\n      layer.push(ZeroPad.fromLevel(level))\n    }\n\n    level += 1\n\n    // If we have 0 nodes in the current layer we just move to the next one.\n\n    // If we have a next layer and we are building  will combine nodes from the current layer\n    next = next ? (build ? [...next] : next) : []\n    let index = 0\n    // Note that we have checked that we have an even number of nodes so\n    // we will never end up with an extra node when consuming two at a time.\n    while (index + 1 < layer.length) {\n      const node = computeNode(layer[index], layer[index + 1])\n\n      // we proactively delete nodes in order to free up a memory used.\n      delete layer[index]\n      delete layer[index + 1]\n\n      next.push(node)\n      index += 2\n    }\n\n    if (next.length) {\n      layers[level] = next\n    }\n\n    // we remove nodes that we have combined from the current layer to reduce\n    // memory overhead and move to the next layer.\n    layer.splice(0, index)\n  }\n\n  return layers\n}\n"],"mappings":"AAAA,OAAO,KAAKA,GAAG,MAAM,UAAU;AAC/B,SACEC,iBAAiB,EACjBC,UAAU,EACVC,WAAW,EACXC,gBAAgB,QACX,eAAe;AACtB,OAAO,KAAKC,OAAO,MAAM,gBAAgB;AACzC,SAASC,WAAW,QAAQ,YAAY;AACxC,SAASC,KAAK,QAAQ,iBAAiB;AACvC,SAASC,GAAG,QAAQ,WAAW;AAC/B,SAASC,UAAU,IAAIC,kBAAkB,QAAQ,0BAA0B;AAC3E,SAASC,QAAQ,QAAQ,iBAAiB;AAC1C,OAAO,KAAKC,MAAM,MAAM,aAAa;AACrC,SAASC,MAAM,QAAQ,cAAc;AAErC,SAASD,MAAM;;AAEf;AACA;AACA;AACA,OAAO,MAAME,IAAI,GAAG;AAClB,2CACD;;AAED;AACA;AACA;AACA;AACA,OAAO,MAAMC,IAAI,GAAG,MAAM;;AAE1B;AACA;AACA;AACA;AACA;AACA,OAAO,MAAMC,UAAU,GAAG,GAAG;;AAE7B;AACA;AACA;AACA;AACA;AACA,OAAO,MAAMC,gBAAgB,GAC1BP,kBAAkB,CAACM,UAAU,CAAC,GAAGE,MAAM,CAAChB,UAAU,CAAC,GAAIgB,MAAM,CAACf,WAAW,CAAC;;AAE7E;AACA;AACA;AACA;AACA;AACA;AACA,OAAO,MAAMgB,MAAM,GAAIC,OAAO,IAAK;EACjC,MAAMC,MAAM,GAAG,IAAIC,MAAM,CAAC,CAAC;EAC3BD,MAAM,CAACE,KAAK,CAACH,OAAO,CAAC;EACrB,OAAOC,MAAM,CAACF,MAAM,CAAC,CAAC;AACxB,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA,OAAO,MAAMK,MAAM,GAAGA,CAAA,KAAM,IAAIF,MAAM,CAAC,CAAC;;AAExC;AACA;AACA;AACA;AACA;AACA,MAAMA,MAAM,CAAC;EACXG,WAAWA,CAAA,EAAG;IACZ;AACJ;AACA;AACA;AACA;IACI,IAAI,CAACC,YAAY,GAAG,EAAE;;IAEtB;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;IACI,IAAI,CAACC,MAAM,GAAG,IAAIC,UAAU,CAAC3B,iBAAiB,CAAC;;IAE/C;AACJ;AACA;AACA;AACA;AACA;AACA;IACI,IAAI,CAAC4B,MAAM,GAAG,CAAC;;IAEf;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;IACI,IAAI,CAACC,MAAM,GAAG,CAAC,EAAE,CAAC;EACpB;;EAEA;AACF;AACA;AACA;AACA;AACA;EACEC,KAAKA,CAAA,EAAG;IACN,OAAO,IAAI,CAACL,YAAY;EAC1B;;EAEA;AACF;AACA;AACA;AACA;AACA;EACEP,MAAMA,CAAA,EAAG;IACP,MAAMa,KAAK,GAAG,IAAIJ,UAAU,CAAChB,MAAM,CAACqB,QAAQ,CAAC;IAC7C,MAAMF,KAAK,GAAG,IAAI,CAACG,UAAU,CAACF,KAAK,EAAE,CAAC,EAAE,IAAI,CAAC;IAC7C,OAAOpB,MAAM,CAACuB,SAAS,CAACH,KAAK,CAACI,QAAQ,CAAC,CAAC,EAAEL,KAAK,CAAC,CAAC;EACnD;;EAEA;AACF;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;EACEG,UAAUA,CAACG,MAAM,EAAEC,UAAU,GAAG,CAAC,EAAEC,WAAW,GAAG,IAAI,EAAE;IACrD,MAAM;MAAEZ,MAAM;MAAEG,MAAM;MAAED,MAAM;MAAEH;IAAa,CAAC,GAAG,IAAI;;IAErD;IACA;IACA,IAAI,CAACc,MAAM,EAAE,GAAGC,KAAK,CAAC,GAAGX,MAAM;;IAE/B;IACA;IACA;IACA,IAAID,MAAM,GAAG,CAAC,IAAIH,YAAY,KAAK,EAAE,EAAE;MACrCc,MAAM,GAAG,CAAC,GAAGA,MAAM,EAAE,GAAGjC,KAAK,CAACC,GAAG,CAACmB,MAAM,CAACe,IAAI,CAAC,CAAC,EAAEb,MAAM,CAAC,CAAC,CAAC,CAAC;IAC7D;IAEA,MAAMc,IAAI,GAAGC,KAAK,CAAC,CAACJ,MAAM,EAAE,GAAGC,KAAK,CAAC,CAAC;IACtC,MAAMI,MAAM,GAAGF,IAAI,CAACG,MAAM,GAAG,CAAC;IAC9B,MAAM,CAACC,IAAI,CAAC,GAAGJ,IAAI,CAACE,MAAM,CAAC;IAC3B,MAAMG,OAAO,GAAGC,MAAM,CAACtC,QAAQ,CAACuC,SAAS,CAAC,IAAI,CAACxB,YAAY,CAAC,CAAC;IAE7D,MAAMyB,aAAa,GAAGtC,MAAM,CAACuC,cAAc,EACzC,8BAAgCJ,OAClC,CAAC;IAED,IAAIK,SAAS,GAAGf,UAAU;IAC1B;IACA,IAAIC,WAAW,EAAE;MACf1B,MAAM,CAACyC,QAAQ,CAACvC,IAAI,EAAEsB,MAAM,EAAEgB,SAAS,CAAC;MACxCA,SAAS,IAAIzC,MAAM,CAAC2C,QAAQ;MAE5B,MAAMC,IAAI,GAAGL,aAAa,GAAGvC,MAAM,CAAC6C,WAAW,GAAG7C,MAAM,CAAC8C,SAAS;MAClE,MAAMC,UAAU,GAAG9C,MAAM,CAACuC,cAAc,CAACI,IAAI,CAAC;MAC9C3C,MAAM,CAACyC,QAAQ,CAACE,IAAI,EAAEnB,MAAM,EAAEgB,SAAS,CAAC;MACxCA,SAAS,IAAIM,UAAU;IACzB;IAEA9C,MAAM,CAACyC,QAAQ,CAACN,OAAO,EAAEX,MAAM,EAAEgB,SAAS,CAAC;IAC3CA,SAAS,IAAIF,aAAa;;IAE1B;IACAd,MAAM,CAACgB,SAAS,CAAC,GAAGR,MAAM;IAC1BQ,SAAS,IAAI,CAAC;;IAEd;IACAhB,MAAM,CAACuB,GAAG,CAACb,IAAI,EAAEM,SAAS,CAAC;IAC3BA,SAAS,IAAIN,IAAI,CAACD,MAAM;;IAExB;IACA,OAAOO,SAAS,GAAGf,UAAU;EAC/B;EACA;AACF;AACA;EACEf,KAAKA,CAACS,KAAK,EAAE;IACX,MAAM;MAAEL,MAAM;MAAEE,MAAM;MAAEC;IAAO,CAAC,GAAG,IAAI;IACvC,MAAMU,MAAM,GAAGV,MAAM,CAAC,CAAC,CAAC;IACxB,MAAM;MAAEgB;IAAO,CAAC,GAAGd,KAAK;IACxB;IACA,IAAIc,MAAM,KAAK,CAAC,EAAE;MAChB,OAAO,IAAI;MACX;IACF,CAAC,MAAM,IAAI,IAAI,CAACpB,YAAY,GAAGR,MAAM,CAAC4B,MAAM,CAAC,GAAG7B,gBAAgB,EAAE;MAChE,MAAM,IAAI4C,UAAU,CACjB,WAAUf,MAAO,sCAAqC7B,gBAAiB,EAC1E,CAAC;IACH;IACA;IACA;IAAA,KACK,IAAIY,MAAM,GAAGiB,MAAM,GAAGnB,MAAM,CAACmB,MAAM,EAAE;MACxCnB,MAAM,CAACiC,GAAG,CAAC5B,KAAK,EAAEH,MAAM,CAAC;MACzB,IAAI,CAACA,MAAM,IAAIiB,MAAM;MACrB,IAAI,CAACpB,YAAY,IAAIR,MAAM,CAAC4B,MAAM,CAAC;MACnC,OAAO,IAAI;IACb;IACA;IACA;IACA;IACA;IAAA,KACK;MACH;MACA,MAAMgB,aAAa,GAAGnC,MAAM,CAACmB,MAAM,GAAGjB,MAAM;MAC5C;MACA;MACAF,MAAM,CAACiC,GAAG,CAAC5B,KAAK,CAACI,QAAQ,CAAC,CAAC,EAAE0B,aAAa,CAAC,EAAEjC,MAAM,CAAC;MACpDW,MAAM,CAACuB,IAAI,CAAC,GAAGxD,KAAK,CAACC,GAAG,CAACmB,MAAM,CAAC,CAAC,CAAC;;MAElC;MACA;MACA,IAAIqC,UAAU,GAAGF,aAAa;MAC9B,OAAOE,UAAU,GAAG/D,iBAAiB,GAAG6C,MAAM,EAAE;QAC9C,MAAMmB,IAAI,GAAGjC,KAAK,CAACI,QAAQ,CAAC4B,UAAU,EAAEA,UAAU,GAAG/D,iBAAiB,CAAC;QACvEuC,MAAM,CAACuB,IAAI,CAAC,GAAGxD,KAAK,CAACC,GAAG,CAACyD,IAAI,CAAC,CAAC,CAAC;QAChCD,UAAU,IAAI/D,iBAAiB;MACjC;;MAEA;MACA;MACA,IAAI,CAAC0B,MAAM,CAACiC,GAAG,CAAC5B,KAAK,CAACI,QAAQ,CAAC4B,UAAU,CAAC,EAAE,CAAC,CAAC;MAC9C,IAAI,CAACnC,MAAM,GAAGiB,MAAM,GAAGkB,UAAU;;MAEjC;MACA,IAAI,CAACtC,YAAY,IAAIR,MAAM,CAAC4B,MAAM,CAAC;;MAEnC;MACAoB,KAAK,CAAC,IAAI,CAACpC,MAAM,CAAC;MAElB,OAAO,IAAI;IACb;EACF;;EAEA;AACF;AACA;AACA;EACEqC,KAAKA,CAAA,EAAG;IACN,IAAI,CAACtC,MAAM,GAAG,CAAC;IACf,IAAI,CAACH,YAAY,GAAG,EAAE;IACtB,IAAI,CAACI,MAAM,CAACgB,MAAM,GAAG,CAAC;IACtB,IAAI,CAAChB,MAAM,CAAC,CAAC,CAAC,CAACgB,MAAM,GAAG,CAAC;IACzB,OAAO,IAAI;EACb;;EAEA;EACAsB,OAAOA,CAAA,EAAG;IACR,IAAI,CAACD,KAAK,CAAC,CAAC;EACd;EACA,IAAIpD,IAAIA,CAAA,EAAG;IACT,OAAOA,IAAI;EACb;EACA,IAAID,IAAIA,CAAA,EAAG;IACT,OAAOA,IAAI;EACb;AACF;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,MAAMoD,KAAK,GAAIpC,MAAM,IAAKuC,KAAK,CAACvC,MAAM,EAAE,KAAK,CAAC;;AAE9C;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,MAAMc,KAAK,GAAId,MAAM,IAAKuC,KAAK,CAAC,CAAC,GAAGvC,MAAM,CAAC,EAAE,IAAI,CAAC;;AAElD;AACA;AACA;AACA;AACA;AACA,MAAMuC,KAAK,GAAGA,CAACvC,MAAM,EAAEc,KAAK,KAAK;EAC/B;EACA;EACA;EACA,IAAI0B,KAAK,GAAG,CAAC;EACb;EACA;EACA,OAAOA,KAAK,GAAGxC,MAAM,CAACgB,MAAM,EAAE;IAC5B,IAAIyB,IAAI,GAAGzC,MAAM,CAACwC,KAAK,GAAG,CAAC,CAAC;IAC5B,MAAME,KAAK,GAAG1C,MAAM,CAACwC,KAAK,CAAC;;IAE3B;IACA;IACA,IAAI1B,KAAK,IAAI4B,KAAK,CAAC1B,MAAM,GAAG,CAAC,GAAG,CAAC,IAAIyB,IAAI,EAAE;MACzCC,KAAK,CAACT,IAAI,CAAC1D,OAAO,CAACoE,SAAS,CAACH,KAAK,CAAC,CAAC;IACtC;IAEAA,KAAK,IAAI,CAAC;;IAEV;;IAEA;IACAC,IAAI,GAAGA,IAAI,GAAI3B,KAAK,GAAG,CAAC,GAAG2B,IAAI,CAAC,GAAGA,IAAI,GAAI,EAAE;IAC7C,IAAIG,KAAK,GAAG,CAAC;IACb;IACA;IACA,OAAOA,KAAK,GAAG,CAAC,GAAGF,KAAK,CAAC1B,MAAM,EAAE;MAC/B,MAAM6B,IAAI,GAAGrE,WAAW,CAACkE,KAAK,CAACE,KAAK,CAAC,EAAEF,KAAK,CAACE,KAAK,GAAG,CAAC,CAAC,CAAC;;MAExD;MACA,OAAOF,KAAK,CAACE,KAAK,CAAC;MACnB,OAAOF,KAAK,CAACE,KAAK,GAAG,CAAC,CAAC;MAEvBH,IAAI,CAACR,IAAI,CAACY,IAAI,CAAC;MACfD,KAAK,IAAI,CAAC;IACZ;IAEA,IAAIH,IAAI,CAACzB,MAAM,EAAE;MACfhB,MAAM,CAACwC,KAAK,CAAC,GAAGC,IAAI;IACtB;;IAEA;IACA;IACAC,KAAK,CAACI,MAAM,CAAC,CAAC,EAAEF,KAAK,CAAC;EACxB;EAEA,OAAO5C,MAAM;AACf,CAAC","ignoreList":[]},"metadata":{},"sourceType":"module","externalDependencies":[]}